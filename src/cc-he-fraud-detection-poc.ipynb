{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c6dfba",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Homomorphic Encryption Neural Network\n",
    "\n",
    "This notebook is a proof of concept of running a credit card (CC) fraud detection neural network on encrypted data. A plaintext neural network is first trained using [PyTorch](https://pytorch.org/), and then a private network uses these weights to perform homomorphic encryption (HE) operations with [TenSEAL](https://github.com/OpenMined/TenSEAL). Included in the aims were to keep computation costs down by using a low-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7b5b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import tenseal as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1e0e8",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used is the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset created by Machine Learning Group - ULB. It consists of 284,807 transactions, 492 of which are fraudulant. \n",
    "\n",
    "Each transaction has 30 pieces of information, and a Class of 0 for *valid* or 1 for *fraudulant*. All column names other than Time and Amount have been obfuscated for privacy. In our case, the dataset may not need to have obfuscated column names, as the network could potentially be trained on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b517ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "\n",
      "[1 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Show the first row of the dataset\n",
    "data = pd.read_csv('../data/creditcard.csv')\n",
    "print(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08275263",
   "metadata": {},
   "source": [
    "The percentage of fraudulant transactions in the dataset is very low, which is something we may consider in the design of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e577c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraudulant transactions: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# The percentage of fraudulant transactions in the dataset is very low, which is something we may consider in the\n",
    "# design of our network.\n",
    "plt.hist(data['Class'], color='red')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Transaction')\n",
    "plt.title('Class Imbalance', fontsize=15)\n",
    "plt.show()\n",
    "pc_fraud = len(data.loc[data['Class'] == 1].values)/len(data.loc[data['Class'] == 0].values)\n",
    "print('Percentage of fraudulant transactions: {:.2f}%'.format(pc_fraud*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d06e05",
   "metadata": {},
   "source": [
    "The data is then preprocessed and split into a dataset for training the network and a dataset for testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172311e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of each dataset:\n",
      "Train: Counter({0: 213207, 1: 398})\n",
      "Test: Counter({0: 71108, 1: 94})\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data by scaling into a [0,1] range and splitting into inputs x and outputs y\n",
    "x = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "# Splitting the data into a dataset for training the network and a dataset for testing it\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, shuffle=False)\n",
    "\n",
    "print('Distribution of each dataset:')\n",
    "print('Train: %s' % Counter(ytrain))\n",
    "print('Test: %s' % Counter(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a586e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders setup, batch size of 100\n",
    "train_ds = torch.utils.data.TensorDataset(torch.tensor(xtrain).float(), torch.tensor(ytrain).float())\n",
    "test_ds = torch.utils.data.TensorDataset(torch.tensor(xtest).float(), torch.tensor(ytest).float())\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=100)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb374438",
   "metadata": {},
   "source": [
    "## Plaintext PyTorch model for training\n",
    "\n",
    "A simple feedforward classifier neural network is trained on the training set using PyTorch, so that we can find the optimum weights to use in our HE Model. This is referred to as the *Plaintext Model*. Other options for neural networks include autoencoders. Out of all the low-layer networks found online trained on this dataset, a simple feedforward classifier seemed to perform best.\n",
    "\n",
    "**Network shape:**\n",
    "* input size is (1, 30)\n",
    "* hidden linear layer of size (30, 15)\n",
    "* output linear layer of size (15, 1)\n",
    "* output size is (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ceeeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plaintext Model\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = torch.nn.Linear(30, 15)\n",
    "#         self.extra_layer = torch.nn.Linear(15, 15)\n",
    "        self.output_layer = torch.nn.Linear(15, 1)\n",
    "\n",
    "    def forward(self, x, show=False):\n",
    "        self.show = show\n",
    "        self.debug_output('Input', x)\n",
    "        y = self.hidden_layer(x)\n",
    "        self.debug_output('Hidden Layer', y)\n",
    "        y = y * y\n",
    "        self.debug_output('Activation', y)\n",
    "#         y = self.extra_layer(y)\n",
    "#         self.debug_output('Extra Layer', y)\n",
    "#         y = y * y\n",
    "#         self.debug_output('Activation', y)\n",
    "        y = self.output_layer(y)\n",
    "        self.debug_output('Output Layer', y)\n",
    "        return y.squeeze()\n",
    "    \n",
    "    def debug_output(self, msg, vec):\n",
    "        if self.show:\n",
    "            print(msg)\n",
    "            print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf8870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b441e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plaintext Model\n",
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f3bf5",
   "metadata": {},
   "source": [
    "The Plaintext Model is trained for 100 epochs on the training set. Once training has been completed once, saved model weights can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e99d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03620145816437772\n",
      "1 0.034462346028963385\n",
      "2 0.032744646682713846\n",
      "3 0.031029572391020357\n",
      "4 0.029341918457876457\n",
      "5 0.02772065318370824\n",
      "6 0.026198802035318037\n",
      "7 0.02479645866328406\n",
      "8 0.023521110563197344\n",
      "9 0.022369142298927978\n",
      "10 0.021325415209874254\n",
      "11 0.0203591007958788\n",
      "12 0.019411688148663023\n",
      "13 0.018374815407198347\n",
      "14 0.017098155294440278\n",
      "15 0.015596058948103314\n",
      "16 0.014230438389666984\n",
      "17 0.013236606546258906\n",
      "18 0.01256473646832299\n",
      "19 0.012108482543376924\n",
      "20 0.011789128016410508\n",
      "21 0.011557204249997191\n",
      "22 0.011382647604949012\n",
      "23 0.011247020361063231\n",
      "24 0.011138746682369924\n",
      "25 0.011050311568851088\n",
      "26 0.010976689393582319\n",
      "27 0.01091438268489419\n",
      "28 0.010860888793925376\n",
      "29 0.010814351690051254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mpos_weight)\n\u001b[1;32m      5\u001b[0m n_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, model, loss_func, opt, train_dl, valid_dl)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m      6\u001b[0m         loss_batch(model, loss_func, xb, yb, opt)\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/dataset.py:369\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nuggi/fyp/cc-he-poc/src/env/lib/python3.8/site-packages/torch/utils/data/dataset.py:369\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training (commented out once training is done and model weights are saved)\n",
    "pos_weight = torch.tensor([5])\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "n_epoch = 100\n",
    "\n",
    "train(n_epoch,model,loss_func,opt,train_dl,test_dl)\n",
    "# torch.save(model.hidden_layer, './data/fhe-compatible-model-hidden-layer.pt')\n",
    "# torch.save(model.output_layer, './data/fhe-compatible-model-output-layer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.hidden_layer = torch.load('./data/fhe-compatible-model-hidden-layer.pt')\n",
    "# model.output_layer = torch.load('./data/fhe-compatible-model-output-layer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41379bfb",
   "metadata": {},
   "source": [
    "An example of one of the datapoints in the test set passing through the Plaintext Model is now shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(xtest[0]).float(), show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b33eb",
   "metadata": {},
   "source": [
    "## Private Model using Homomorphic Encryption\n",
    "\n",
    "Model which uses HE operations for each layer. It has the same shape as the Plaintext Model so it can be initialised with the weights found in training. The encryption, matrix multiplications, addition and decryption are done using TenSEAL. This network is referred to as the *HE Model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aea2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HE Model\n",
    "class HEModel:\n",
    "    def __init__(self, hidden_layer, output_layer):\n",
    "        self.hidden_layer_weight = hidden_layer.weight.t().tolist()\n",
    "        self.hidden_layer_bias = hidden_layer.bias.tolist()\n",
    "#         self.extra_layer_weight = extra_layer.weight.t().tolist()\n",
    "#         self.extra_layer_bias = extra_layer.bias.tolist()\n",
    "        self.output_layer_weight = output_layer.weight.t().tolist()\n",
    "        self.output_layer_bias = output_layer.bias.tolist()\n",
    "        \n",
    "    def forward(self, enc_x, show=False, plaintext=False):\n",
    "        self.show = show\n",
    "        self.plaintext = plaintext\n",
    "        self.debug_output('Input', enc_x)\n",
    "        enc_y = enc_x.mm(self.hidden_layer_weight) + self.hidden_layer_bias\n",
    "        self.debug_output('Hidden Layer', enc_y)\n",
    "        enc_y *= enc_y\n",
    "        self.debug_output('Activation', enc_y)\n",
    "#         enc_y = ts.ckks_vector(context, enc_y.decrypt())\n",
    "#         enc_y = enc_y.mm(self.extra_layer_weight) + self.extra_layer_bias\n",
    "#         self.debug_output('Extra Layer', enc_y)\n",
    "#         enc_y *= enc_y\n",
    "#         self.debug_output('Activation', enc_y)\n",
    "        enc_y = enc_y.mm(self.output_layer_weight) + self.output_layer_bias\n",
    "        self.debug_output('Output Layer', enc_y)\n",
    "        return enc_y\n",
    "    \n",
    "    def debug_output(self, msg, vec):\n",
    "        if self.show:\n",
    "            print(msg)\n",
    "            if self.plaintext: vec = torch.tensor(vec.decrypt())\n",
    "            print(vec)\n",
    "    \n",
    "    def __call__(self, enc_x, show=False, plaintext=False):\n",
    "        return self.forward(enc_x, show, plaintext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda19c9d",
   "metadata": {},
   "source": [
    "The CKKS HE scheme is used. This allows for operations to be done on full tensors at a time.\n",
    "\n",
    "More information on CKKS can be found in OpenMined's [CKKS Explained Series](https://blog.openmined.org/ckks-explained-part-1-simple-encoding-and-decoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TenSEAL CKKS HE encryption scheme context setup\n",
    "bits_scale = 50\n",
    "coeff_mod_bit_sizes = [60, bits_scale, bits_scale, bits_scale, 60]\n",
    "polynomial_modulus_degree = 8192*2\n",
    "\n",
    "# Create context\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, polynomial_modulus_degree, coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
    "# Set global scale\n",
    "context.global_scale = 2**bits_scale\n",
    "# Generate galois keys required for matmul in ckks_vector\n",
    "context.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1933b",
   "metadata": {},
   "source": [
    "The HE Model is initialised with the weights acquired from the training of the Plaintext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e63f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_model = HEModel(model.hidden_layer, model.output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba4576",
   "metadata": {},
   "source": [
    "An example of one of the datapoints in the test set being encrypted and then passed through the HE Model is now shown. As the layers in the model use HE, each output is an encrypted CKKS vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of encrypted test datapoint passing through the HE Model\n",
    "enc_input = ts.ckks_vector(context, xtest[0])\n",
    "he_model(enc_input, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6f7d8",
   "metadata": {},
   "source": [
    "For demonstration purposes, a decrypted version of this debug output is shown in order to sanity check that the data is passing through the HE Model the same way it passes through the Plaintext Model. When compared with the previously shown output of the Plaintext Model, it is confirmed that they are the same.\n",
    "\n",
    "In a production environment, the server running the network would not have the private key required to decrypt the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of encrypted test datapoint passing through the HE Model, showing each stage decrypted\n",
    "he_model(enc_input, show=True, plaintext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bc140",
   "metadata": {},
   "source": [
    "HE operations are much slower than standard plaintext operations. The time difference of an inference using the Plaintext Model and the HE Model are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f57b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time difference between Plaintext Model and HE Model\n",
    "%time model(torch.tensor(xtest[0]).float())\n",
    "%time he_model(enc_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938549d",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To validate that our Plaintext Model and our HE Model output the same values, we will evaluate a small subset of our test set with each network. Evaluating the full test set with our HE Model would take a lot of time, so we can leave the evaluation of the full test set to the Plaintext Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate_predictions(preds, truths, threshold=0.5):\n",
    "    print('AUPRC score: {}'.format(metrics.average_precision_score(truths, preds)))\n",
    "    print('AUROC score: {}'.format(metrics.roc_auc_score(truths, preds)))\n",
    "    preds[preds>=threshold] = 1.0\n",
    "    preds[preds<threshold] = 0.0\n",
    "    conf_matrix = metrics.confusion_matrix(truths, preds)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    labels = [\"Valid\", \"Fraud\"]\n",
    "    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    print('Accuracy score: {}'.format(metrics.accuracy_score(truths, preds)))\n",
    "    print(metrics.classification_report(truths, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e73a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll take the indices of all the fraudulant transactions in the test set\n",
    "fraud_test_idxs = ytest==1\n",
    "# And combine them with an extra 100 transactions\n",
    "fraud_test_idxs[:100] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc026c9",
   "metadata": {},
   "source": [
    "Evaluation of the sample with the Plaintext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the sample with the Plaintext Model\n",
    "results_plain = []\n",
    "for x in xtest[fraud_test_idxs]:\n",
    "    results_plain.append(model(torch.tensor(x).float()).detach().numpy())\n",
    "results_plain = np.array(results_plain).squeeze()\n",
    "\n",
    "evaluate_predictions(results_plain, ytest[fraud_test_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62cf93",
   "metadata": {},
   "source": [
    "The most important intuitive metric shown above is the *Confusion matrix* which shows in this case that all 100 valid transactions were successfully labelled as valid, and 79 of the 101 fraudulant transactions were successfully labelled as fraudulant.\n",
    "\n",
    "Next the sample is evaluated with the HE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the sample with the HE Model\n",
    "results_encrypted = []\n",
    "for x in xtest[fraud_test_idxs]:\n",
    "    enc_input = ts.ckks_vector(context, x)\n",
    "    enc_output = he_model(enc_input)\n",
    "    res = enc_output.decrypt()\n",
    "    results_encrypted.append(res)\n",
    "results_encrypted = np.array(results_encrypted).squeeze()\n",
    "\n",
    "evaluate_predictions(results_encrypted, ytest[fraud_test_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fe080",
   "metadata": {},
   "source": [
    "The same results are found. This demonstrates that the Plaintext Model and teh HE Model both output the same values.\n",
    "\n",
    "Finally, the full test set is evaluated using the Plaintext Model. This will demonstrate the performance of the weights used in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the full test set with the Plaintext Model\n",
    "results_plain = []\n",
    "for x in xtest:\n",
    "    results_plain.append(model(torch.tensor(x).float()).detach().numpy())\n",
    "results_plain = np.array(results_plain).squeeze()\n",
    "\n",
    "evaluate_predictions(results_plain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa9ed7",
   "metadata": {},
   "source": [
    "As shown, 56847 of the 56861 valid transactions are successfully labelled as valid, and 79 of the 101 fraudulant transactions are successfully labelled as fraudulant.\n",
    "\n",
    "Although these results are not perfect, it concludes the demonstration of how a model can be trained using PyTorch to identify fraudulant CC transactions and then converted into a model that can operate on encrypted inputs using HE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863f81c",
   "metadata": {},
   "source": [
    "Some functions for training and evaluation were adapted from this [Kaggle notebook](https://www.kaggle.com/rinabuoy/credit-card-fraud-detection-with-pytorch/notebook) on CC fraud detection.\n",
    "\n",
    "The HE network is adapted from this official [TenSEAL tutorial](https://github.com/OpenMined/courses/blob/foundations-of-private-computation/homomorphic-encryption/Evaluation%20on%20Encrypted%20Data%20using%20TenSEAL%20-%20Solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp2",
   "language": "python",
   "name": "fyp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
